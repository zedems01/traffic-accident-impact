{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing with PySpark on AWS EMR\n",
    "\n",
    "The initial phase of this project involved preprocessing the US car accident dataset, which contains approximately 7.7 million records spanning from February 2016 to March 2023 across 49 states. Given the dataset’s size and complexity, we utilized an AWS EMR (Elastic MapReduce) cluster with PySpark to efficiently handle the data processing tasks. The preprocessing steps were designed to clean and transform the raw data into a suitable format for subsequent analysis and modeling, specifically to predict accident duration and its impact on traffic flow.\n",
    "\n",
    "The preprocessing workflow consisted of several key steps. First, we loaded the dataset from a CSV file stored in an S3 bucket and standardized the column names to lowercase for consistency. Unnecessary columns—such as `id`, `end_lat`, `end_lng`, and `wind_chill(f)`—were removed to streamline the dataset. Next, we addressed missing values in critical columns like `precipitation(in)` and `wind_speed(mph)` by filling them with monthly averages calculated based on the accident start time. This ensured that gaps in weather-related data were handled contextually rather than left as nulls. Rows with remaining null values across any column were then dropped to maintain data integrity.\n",
    "\n",
    "To prepare the dataset for time-based analysis, we processed the `start_time` and `end_time` columns by converting them to timestamps and calculating the accident duration in minutes. Additionally, we extracted temporal features—start hour, day, month, and year—from the `start_time` to enrich the dataset with time-related insights. These features are essential for understanding patterns in accident duration and traffic impact.\n",
    "\n",
    "Outlier detection and removal were also critical steps to ensure the dataset’s reliability. We focused on key numerical columns—`distance(mi)`, `temperature(f)`, `pressure(in)`, `visibility(mi)`, `wind_speed(mph)`, `precipitation(in)`, and the newly computed `accident_duration(min)`—and computed whiskers (lower and upper bounds) using the interquartile range (IQR) method with approximate quantiles. Records falling outside these bounds were filtered out to eliminate extreme values that could skew the analysis.\n",
    "\n",
    "Finally, the preprocessed dataset was saved as a Parquet file in an S3 bucket, coalesced into a single partition for simplicity. The process was logged throughout, with the log file uploaded alongside the output data for transparency and debugging purposes. This preprocessing effort reduced noise, enhanced data quality, and set the stage for accurate modeling of accident duration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Processing with Doc2Vec\n",
    "\n",
    "Following the initial preprocessing with PySpark, the next step involved handling the textual data within the dataset, specifically the `description` column containing details about each accident. Given the large volume of data—approximately 7.7 million records—using advanced embedding models like OpenAI’s `text-embedding-ada-002` was deemed impractical due to computational and cost constraints. Instead, we adopted a traditional yet effective vectorization approach using `Doc2Vec`, a model from the Gensim library, to transform the text into numerical representations suitable for downstream modeling.\n",
    "\n",
    "The text processing workflow began by loading the preprocessed dataset from a Parquet file generated by the PySpark job on AWS EMR. We first normalized the categorical features to ensure consistency across the dataset. The `visibility(mi)` column was dropped as it was deemed non-essential for this phase. Integer-based columns—such as `severity`, `start_hour`, `start_day`, `start_month`, and `start_year`—were converted to a categorical type, while object and boolean columns were similarly transformed. All categorical values were then standardized by converting them to lowercase and stripping whitespace.\n",
    "\n",
    "The core of the text processing focused on the `description` column. We tokenized the text by converting it to lowercase, removing punctuation, and splitting it into individual words. Non-alphabetical tokens were filtered out, and the remaining words were processed by removing stopwords (common words like \"the\" or \"and\") and applying lemmatization to reduce words to their root form (e.g., \"running\" to \"run\"). This resulted in a clean set of tokens for each accident description, ready for vectorization.\n",
    "\n",
    "To convert these tokens into numerical embeddings, we trained a `Doc2Vec` model with a vector size of 100, a window size of 5, and 20 epochs, using the distributed memory (DM) approach. The model was trained on tagged documents, where each tokenized description was paired with a unique identifier. Once trained, the model generated 100-dimensional vectors capturing the semantic meaning of each description. To reduce dimensionality and improve computational efficiency, we applied Principal Component Analysis (PCA) to these vectors, retaining the top three components. These PCA-derived features—labeled `description_pca1`, `description_pca2`, and `description_pca3`—were added to the dataset, while the original `description` and `tokens` columns were discarded.\n",
    "\n",
    "The resulting dataset, now enriched with text embeddings, was saved as a Parquet file for use in subsequent modeling steps. This approach effectively transformed unstructured text into a structured format, enabling the inclusion of accident descriptions in the prediction of traffic flow impact. The process was logged throughout, with logs stored alongside the output for traceability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "After completing the initial preprocessing and text processing phases, it became evident that an additional round of feature engineering was necessary to further refine the dataset for modeling. This step aimed to enhance the predictive power of the features by transforming, grouping, and reducing them, ensuring the dataset was optimized for forecasting accident duration and its impact on traffic flow.\n",
    "\n",
    "The feature engineering process began by loading the dataset from the Parquet file generated in the text processing step. We first streamlined the dataset by dropping a set of columns deemed redundant or irrelevant for the analysis. These included metadata like `weather_timestamp`, `airport_code`, `country`, and `source`, as well as location-specific details such as `street`, `city`, `county`, and `zipcode`. Additionally, binary road condition flags—such as `amenity`, `bump`, `crossing`, and `traffic_signal`—and twilight-related columns were removed to reduce dimensionality. Outliers in the `accident_duration(min)` column were also filtered out to ensure the target variable remained robust.\n",
    "\n",
    "To capture the periodic nature of temporal and spatial features, we applied cyclic encoding using sine and cosine transformations. The `start_lng` (longitude) was encoded with a period of 360 degrees, while `start_hour` (24-hour cycle), `start_day` (7-day week), and `start_month` (12-month year) were similarly transformed. The original columns were then dropped, leaving pairs of sine and cosine features that effectively represent these cyclical patterns without assuming linear relationships. For the `wind_direction` column, textual values (e.g., \"north\", \"east\", \"calm\") were mapped to corresponding angles (in degrees), cyclically encoded with a 360-degree period, and then replaced with their sine and cosine components.\n",
    "\n",
    "Next, we grouped categorical features to simplify the dataset and enhance interpretability. The `state` column, representing the 49 states in the dataset, was categorized into `urban`, `rural`, or `unknown` based on a predefined list of urban and rural states. This reduced the granularity while preserving meaningful distinctions in traffic and accident patterns. Similarly, the `weather_condition` column, which contained a wide range of detailed weather descriptions, was mapped into four broader groups: `clear`, `cloudy`, `precipitation`, and `obscured`. Unmapped conditions were labeled as `unknown`. These groupings reduced noise and consolidated related conditions into more manageable categories.\n",
    "\n",
    "The final dataset, now transformed and enriched, was saved as a Parquet file. This process resulted in a cleaner, more focused set of features, with reduced dimensionality and improved representation of temporal, spatial, and environmental factors. The feature engineering phase ensured the data was well-prepared for modeling, balancing complexity and predictive potential. Logs were maintained throughout the process and stored alongside the output for reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling\n",
    "\n",
    "With the dataset fully preprocessed and enriched through feature engineering, the next phase focused on training predictive models to forecast accident duration and assess its impact on traffic flow. To achieve this, we developed training pipelines for five distinct regression models: Linear Regression, Random Forest, XGBoost, CatBoost, and LightGBM. These pipelines were designed not only to train the models but also to optimize their hyperparameters, evaluate performance, and save the results—including metrics and visualizations—for further analysis.\n",
    "\n",
    "The modeling process began by loading the final preprocessed dataset from a Parquet file. To accommodate potential computational constraints, we allowed for sampling a fraction of the data (defaulting to 100%), ensuring flexibility in experimentation. The dataset was split into features (`X`) and the target variable (`accident_duration(min)`), with an 80-20 train-test split applied to reserve 20% of the data for testing. Numerical features were standardized using a `StandardScaler`, while categorical features—such as `state_group` and `weather_group`—were one-hot encoded (dropping the first category to avoid multicollinearity). This preprocessing was integrated into a `ColumnTransformer` within each pipeline to ensure consistent data preparation.\n",
    "\n",
    "For each model, we employed `RandomizedSearchCV` to tune hyperparameters efficiently, performing a randomized search over predefined parameter distributions with 20 iterations and 5-fold cross-validation. The scoring metric was set to negative root mean squared error (RMSE) to prioritize minimizing prediction errors. The models and their tuned parameters were as follows:\n",
    "- **Linear Regression**: No hyperparameters were tuned, serving as a baseline.\n",
    "- **Random Forest**: Tuned `n_estimators` (100–500), `max_depth` (10–30 or None), and `min_samples_split` (2–10).\n",
    "- **XGBoost**: Tuned `n_estimators` (100–300), `learning_rate` (0.01–0.1), and `max_depth` (4–10).\n",
    "- **CatBoost**: Tuned `iterations` (100–500), `learning_rate` (0.01–0.1), and `depth` (4–10).\n",
    "- **LightGBM**: Tuned `n_estimators` (100–300), `learning_rate` (0.01–0.1), and `max_depth` (6–12).\n",
    "\n",
    "Each pipeline was executed individually, leveraging parallel processing to optimize computational efficiency. After training, the best-performing model (based on cross-validation) was evaluated on both the training and test sets. Key metrics included adjusted R² (accounting for the number of features) and RMSE, providing insights into model fit and prediction accuracy. These metrics were logged for each model, along with the fraction of data used, to facilitate comparison.\n",
    "\n",
    "Beyond performance metrics, we analyzed feature importance and residuals to deepen our understanding of each model’s behavior. For Linear Regression, feature importance was derived from coefficient magnitudes and accompanying p-values (calculated via `statsmodels`), while for tree-based models (Random Forest, XGBoost, CatBoost, LightGBM), it was based on built-in feature importance scores. These results were saved as Excel files. Residual analysis included histograms, scatter plots against predicted values, and Q-Q plots to assess normality and autocorrelation, with the Durbin-Watson statistic computed to detect residual patterns. These visualizations were saved as PNG files for later review.\n",
    "\n",
    "The trained pipelines, optimized with their best hyperparameters, were serialized and saved as `.pkl` files, named according to the model type and data fraction (e.g., `RandomForest-frac-1.0.pkl`). This ensured reproducibility and easy access for future predictions or analysis. Logs captured the entire process, including preparation, training, and evaluation steps, and were stored alongside the outputs.\n",
    "\n",
    "This modeling phase provided a robust framework for comparing multiple regression approaches, balancing simplicity (Linear Regression) with complexity (ensemble methods), and setting the stage for a detailed performance analysis in the next section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R^2$ Train set\n",
    "\n",
    "| Models / frac         | 10% | 30% | 50% | 80% | 100% |\n",
    "|----------------|---------|---------|---------|---------|---------|\n",
    "| Linear         | 0.27    | 0.27    | 0.27    | 0.27    | 0.27    |\n",
    "| XGBoost        | 0.68    | 0.62    | 0.63    | 0.61    | -       |\n",
    "| LightGBM       | 0.54    | 0.53    | 0.53    | 0.53    | -       |\n",
    "| CatBoost       | 0.60    | 0.57    | 0.56    | -       | -       |\n",
    "| RandomForest   | 0.93    | 0.94    | -       | -       | -       |\n",
    "\n",
    "\n",
    "\n",
    "$R^2$ Test set\n",
    "\n",
    "| Models / frac         | 10% | 30% | 50% | 80% | 100% |\n",
    "|----------------|---------|---------|---------|---------|---------|\n",
    "| Linear         | 0.27    | 0.28    | 0.27    | 0.27    | 0.27    |\n",
    "| XGBoost        | 0.53    | 0.55    | 0.55    | 0.56    | -       |\n",
    "| LightGBM       | 0.51    | 0.52    | 0.52    | 0.52    | -       |\n",
    "| CatBoost       | 0.53    | 0.54    | 0.54    | -       | -       |\n",
    "| RandomForest   | 0.53    | 0.55    | -       | -       | -       |\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE Train set\n",
    "\n",
    "| Models / frac         | 10% | 30% | 50% | 80% | 100% |\n",
    "|----------------|---------|---------|---------|---------|---------|\n",
    "| Linear         | 32.18   | 32.17   | 32.13   | 32.14   | 32.15   |\n",
    "| XGBoost        | 21.16   | 23.17   | 22.93   | 23.44   | -       |\n",
    "| LightGBM       | 25.52   | 25.70   | 25.90   | 25.90   | -       |\n",
    "| CatBoost       | 23.60   | 24.70   | 24.90   | -       | -       |\n",
    "| RandomForest   | 9.66    | 9.40    | -       | -       | -       |\n",
    "\n",
    "\n",
    "RMSE Test set\n",
    "\n",
    "| Models / frac         | 10% | 30% | 50% | 80% | 100% |\n",
    "|----------------|---------|---------|---------|---------|---------|\n",
    "| Linear         | 32.14   | 32.10   | 32.14   | 32.11   | 32.15   |\n",
    "| XGBoost        | 25.71   | 25.34   | 25.10   | 24.92   | -       |\n",
    "| LightGBM       | 26.30   | 26.00   | 26.00   | 25.90   | -       |\n",
    "| CatBoost       | 25.80   | 25.50   | 25.40   | -       | -       |\n",
    "| RandomForest   | 25.80   | 25.11   | -       | -       | -       |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "\n",
    "The Linear model shows remarkable consistency. Its $R^2$ remains fixed at 0.27 across all proportions (10% to 100%), with RMSE hovering tightly around 32.13–32.18 on training and 32.10–32.15 on test. This stability suggests the model is too simple to benefit from additional data, as it neither overfits nor adapts to the increased training size. The low $R^2$ (explaining only 27% of variance) and high RMSE (large errors) confirm underfitting, where the model fails to capture the data’s complexity regardless of dataset size.\n",
    "\n",
    "XGBoost, in contrast, demonstrates more dynamic behavior. On the training set, $R^2$ starts high at 0.68 with 10% of the data but dips to 0.61 by 80%, while RMSE rises from 21.16 to 23.44. This could indicate that with less data (10%), XGBoost overfits to the small sample, achieving a better fit, but as more data is added, the fit becomes less tight, possibly due to increased noise or complexity. On the test set, however, $R^2$ improves from 0.53 to 0.56 and RMSE decreases from 25.71 to 24.92 as the proportion grows from 10% to 80%. This trend suggests XGBoost benefits from more training data, improving generalization, though the gap between training and test metrics (e.g., $R^2$ 0.61 vs. 0.56 at 80%) indicates moderate overfitting.\n",
    "\n",
    "LightGBM presents a more stable profile. Its training $R^2$ holds steady at 0.53–0.54 from 10% to 80%, with RMSE increasing slightly from 25.52 to 25.90. Test $R^2$ edges up from 0.51 to 0.52, and RMSE improves marginally from 26.30 to 25.90. The small gap between training and test (e.g., $R^2$ 0.53 vs. 0.52 at 80%) suggests minimal overfitting, and the slight test improvement with more data indicates some benefit from larger training proportions, though less pronounced than XGBoost.\n",
    "\n",
    "CatBoost follows a similar pattern to XGBoost but with fewer data points due to training time limits. Training $R^2$ drops from 0.60 at 10% to 0.56 at 50%, with RMSE rising from 23.60 to 24.90, suggesting a tighter fit with less data. Test $R^2$ improves from 0.53 to 0.54, and RMSE decreases from 25.80 to 25.40 as the proportion increases to 50%. The moderate gap (e.g., $R^2$ 0.56 vs. 0.54 at 50%) points to overfitting, but the test improvement with more data mirrors XGBoost’s trend, hinting at potential for better performance if trained on larger proportions.\n",
    "\n",
    "RandomForest stands out with extreme results, limited to 10% and 30% due to computational cost. Training $R^2$ is exceptionally high (0.93–0.94) and RMSE very low (9.40–9.66), reflecting an almost perfect fit to the small training sets. However, test $R^2$ (0.53–0.55) and RMSE (25.11–25.80) are far worse, revealing severe overfitting. The slight test improvement from 10% to 30% (RMSE 25.80 to 25.11) suggests more data helps marginally, but the large train-test disparity indicates RandomForest is memorizing the training data rather than learning generalizable patterns.\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "Comparing the models, the Linear model’s underfitting is evident in its inability to improve with more data, stuck at a low $R^2$ and high RMSE. XGBoost, LightGBM, and CatBoost all show moderate overfitting, with training metrics outperforming test, but they benefit from larger proportions, especially XGBoost, which achieves the best test performance ($R^2$ 0.56, RMSE 24.92 at 80%). LightGBM’s stability and lower overfitting make it a reliable alternative, while CatBoost’s trajectory suggests potential if extended to higher proportions. RandomForest’s severe overfitting renders it impractical despite its training prowess, likely exacerbated by the small dataset sizes tested.\n",
    "\n",
    "The trend across proportions reveals that more data generally improves test performance for the tree-based models (XGBoost, LightGBM, CatBoost), reducing RMSE and boosting $R^2$, though training performance sometimes weakens as overfitting decreases. RandomForest might follow this trend if trained on more data, but its computational cost limited exploration. The Linear model’s indifference to dataset size underscores its inadequacy for this task.\n",
    "\n",
    "For a recommendation, XGBoost emerges as the strongest candidate, offering the best test metrics at 80% of the data. Its ability to leverage larger proportions suggests it could improve further at 100%, though training time was a constraint. LightGBM provides a stable, less overfitted option, while RandomForest requires regularization or more data to become viable. Given the modest test $R^2$ (max 0.56) and high RMSE (min 24.92), exploring feature engineering or ensemble methods could enhance performance across all models, as the current results suggest untapped potential in the data.\n",
    "\n",
    "\n",
    "#### Future Improvements\n",
    "\n",
    "For future improvements, several strategies could enhance the performance of these models given the current results and the constraint of training time with varying dataset proportions. First, optimizing hyperparameters for XGBoost, LightGBM, and CatBoost—such as adjusting learning rates, tree depth, or regularization parameters—could reduce overfitting and improve test metrics, especially since XGBoost showed the best generalization at 80% of the data. Second, addressing RandomForest’s severe overfitting might involve limiting tree complexity (e.g., maximum depth or minimum samples per leaf) or testing it on larger proportions, potentially using distributed computing to mitigate training time issues. Third, feature engineering or selection could boost all models’ ability to explain more variance, as the highest test $R^2$ (0.56) and lowest RMSE (24.92) suggest the current features leave significant patterns uncaptured. Additionally, experimenting with ensemble techniques, such as stacking XGBoost and LightGBM, could combine their strengths—XGBoost’s predictive power and LightGBM’s stability—for better outcomes. Finally, if computational resources allow, training all models on the full dataset (100%) and incorporating cross-validation would provide a more robust assessment of their potential, ensuring that performance gains from additional data are fully realized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
